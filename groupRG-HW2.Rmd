---
title: "Stat 602 Homework 2"
author: "Kiegan Rice and Nate Garton"
date: "Due 3/4/2019"
output: 
  pdf_document: 
    fig_caption: yes
header-includes: \usepackage{float, mathtools, accents}
---


\newcommand{\dbtilde}[1]{\accentset{\approx}{#1}}

```{r global_options, echo = F, include = FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```

```{r packages, echo = F, warning = F, message = F}
library(tidyverse)
library(gridExtra)
library(knitr)
library(cvTools)
library(class)
library(caret)
library(elasticnet)
library(earth)
library(pls)
```

## Problem 1  

### Problem 4.3  

A table of RMSEs for 8-fold cross-validation can be found in \autoref{prob4p3}. 

```{r, echo = F, warning = F, message = F}
prob4p3_data <- read_csv("data/AmesHousingData.csv")
prob4p3_data <- prob4p3_data %>% select(Price, Size, Fireplace, `Bsmt Bath`, Land) %>% mutate(Bsmt_Bath = `Bsmt Bath`) %>% select(-`Bsmt Bath`) %>% mutate(none = 1)

trControl <- trainControl(method = "cv", number = 8)

none_m <- caret::train(form = Price~none, data = prob4p3_data, 
                       method = "lm", preProcess = c("center", "scale"), 
                       metric = "RMSE", trControl = trControl)$results$RMSE

size_m <- caret::train(form = Price~Size, data = prob4p3_data, 
                       method = "lm", preProcess = c("center", "scale"), 
                       metric = "RMSE", trControl = trControl)$results$RMSE

fire_m <- caret::train(form = Price~Fireplace, data = prob4p3_data, 
                       method = "lm", preProcess = c("center", "scale"), 
                       metric = "RMSE", trControl = trControl)$results$RMSE

bsmt_m <- caret::train(form = Price~Bsmt_Bath, data = prob4p3_data, 
                       method = "lm", preProcess = c("center", "scale"), 
                       metric = "RMSE", trControl = trControl)$results$RMSE

land_m <- caret::train(form = Price~Land, data = prob4p3_data, 
                       method = "lm", preProcess = c("center", "scale"), 
                       metric = "RMSE", trControl = trControl)$results$RMSE

size_fire_m <- caret::train(form = Price~Size + Fireplace, data = prob4p3_data, 
                       method = "lm", preProcess = c("center", "scale"), 
                       metric = "RMSE", trControl = trControl)$results$RMSE  

size_bsmt_m <- caret::train(form = Price~Size + Bsmt_Bath, data = prob4p3_data, 
                       method = "lm", preProcess = c("center", "scale"), 
                       metric = "RMSE", trControl = trControl)$results$RMSE

size_land_m <- caret::train(form = Price~Size + Land, data = prob4p3_data, 
                       method = "lm", preProcess = c("center", "scale"), 
                       metric = "RMSE", trControl = trControl)$results$RMSE

fire_bsmt_m <- caret::train(form = Price~Fireplace + Bsmt_Bath, data = prob4p3_data, 
                       method = "lm", preProcess = c("center", "scale"), 
                       metric = "RMSE", trControl = trControl)$results$RMSE

fire_land_m <- caret::train(form = Price~Fireplace + Land, data = prob4p3_data, 
                       method = "lm", preProcess = c("center", "scale"), 
                       metric = "RMSE", trControl = trControl)$results$RMSE

bsmt_land_m <- caret::train(form = Price~Bsmt_Bath + Land, data = prob4p3_data, 
                       method = "lm", preProcess = c("center", "scale"), 
                       metric = "RMSE", trControl = trControl)$results$RMSE

size_fire_bsmt_m <- caret::train(form = Price~Size + Fireplace + Bsmt_Bath, data = prob4p3_data, 
                       method = "lm", preProcess = c("center", "scale"), 
                       metric = "RMSE", trControl = trControl)$results$RMSE


size_fire_land_m <- caret::train(form = Price~Size + Fireplace + Land, data = prob4p3_data, 
                       method = "lm", preProcess = c("center", "scale"), 
                       metric = "RMSE", trControl = trControl)$results$RMSE

fire_bsmt_land_m <- caret::train(form = Price~Fireplace + Bsmt_Bath + Land, data = prob4p3_data, 
                       method = "lm", preProcess = c("center", "scale"), 
                       metric = "RMSE", trControl = trControl)$results$RMSE

all_m <- caret::train(form = Price~Size + Fireplace + Bsmt_Bath + Land, data = prob4p3_data, 
                       method = "lm", preProcess = c("center", "scale"), 
                       metric = "RMSE", trControl = trControl)$results$RMSE

#rec_obj <- recipe(Price~., data = prob4p3_data)
```

\begin{table}[h]
\centering
\caption{8-fold CV Model RMSEs for Problem 4.3}
\begin{tabular}{lc}
\textbf{Model} & \textbf{8-fold CV RMSE} \\ \hline
None & `r format(none_m, scientific = F)` \\ \hline
Size & `r format(size_m, scientific = F)` \\ \hline
Fireplace & `r format(fire_m, scientific = F)` \\ \hline
Bsmt Bath & `r format(bsmt_m, scientific = F)` \\ \hline
Land & `r format(land_m, scientific = F)` \\ \hline
Size, Fireplace & `r format(size_fire_m, scientific = F)` \\ \hline
Size, Bsmt Bath & `r format(size_bsmt_m, scientific = F)` \\ \hline
Size, Land & `r format(size_land_m, scientific = F)` \\ \hline
Fireplace, Bsmt Bath & `r format(fire_bsmt_m, scientific = F)` \\ \hline
Fireplace, Land & `r format(fire_land_m, scientific = F)` \\ \hline
Bsmt Bath, Land & `r format(bsmt_land_m, scientific = F)` \\ \hline
Size, Fireplace, Bsmt Bath & `r format(size_fire_bsmt_m, scientific = F)` \\ \hline
Size, Fireplace, Land & `r format(size_fire_land_m, scientific = F)` \\ \hline
Fireplace, Bsmt Bath, Land & `r format(fire_bsmt_land_m, scientific = F)` \\ \hline
Size, Fireplace, Bsmt Bath, Land & `r format(all_m, scientific = F)`\\ \hline
\label{prob4p3}
\end{tabular}
\end{table}

The lowest RMSE appears to be for the full model which includes Size, Fireplace, Bsmt Bath, and Land.  



### Problem 4.4  

\textbf{Part a}
```{r, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE}
glass <- read.table(file = "data/glass_data_4pt4.txt", sep = ",", strip.white = TRUE)[,-1]
colnames(glass) <- c("RI","Na", "Mg", "Al", "Si","K","Ca","Ba","Fe","type")

glass12 <- subset(x = glass, subset = type %in% c("1","2"))
glass12$type <- as.factor(glass12$type)
# glass12[,1:(ncol(glass12) - 1)] <- scale(x = glass12[,1:(ncol(glass12) - 1)], center = TRUE, scale = TRUE)

k <- seq(from = 1, to = 20, by = 1)
tune_par <- data.frame("k" = k)
fitcontrol <- trainControl(method = "cv", number = 10)
knn_results <- caret::train(form = type ~ ., 
                            data = glass12, 
                            method = "knn", 
                            preProcess = c("center","scale"), 
                            tuneGrid = tune_par,
                            trControl = fitcontrol)

knn_results
```

Based on 10-fold cross-validation, the accuracy is estimated to be highest when $k = 4$.

\textbf{Part b}

The classification rule $\hat{y} = 2 I\left[t(x) \geq k/2\right] + I\left[t(x) < k/2\right]$ is equivalent to $\hat{y} = 2 I\left[t(x)/k \geq 1/2\right] + I\left[t(x)/k < 1/2\right]$. Here, $t(x)/k$ is an estimate of $E\left[ \text{type} = 2|x \right] = P(\text{type} = 2|x)$. Thus, this is directly an estimate of a posterior class probability which accounts for the prior class probabilities. This means that no modification of kNN needs to be made to account for differing prior probabilities.

### Problem 5.1  

```{r prob5p1, echo = F, warning = F, message = F}
prob5p1_mat <- matrix(c(2, 4, 7, 2, 
                        4, 3, 5, 5, 
                        3, 4, 6, 1, 
                        5, 2, 4, 2, 
                        1, 3, 4, 4), nrow = 5, byrow = T)

#Note: Most of this is on 602 HW2 from 2015 (problem 6)  

m2l <- function(matr) {

    printmrow <- function(x) {

        ret <- paste(paste(round(x, 2),collapse = " & "),"\\\\")
        sprintf(ret)
    }

    out <- apply(matr,1,printmrow)
    out2 <- paste("\\begin{bmatrix}",paste(out,collapse=' '),"\\end{bmatrix}")
    return(out2)
}


```

The following are decompositions using the matrix
\begin{displaymath}
X = `r m2l(prob5p1_mat)`
\end{displaymath}



\textbf{Part a}  

```{r, echo = F, warning = F, message = F, results='asis'}
qr_5p1 <- qr(prob5p1_mat)
qr_bases <- qr.Q(qr_5p1)
qr_r <- qr.R(qr_5p1)


svd_5p1 <- svd(prob5p1_mat)
```

The QR decomposition of $X$ is: 

\begin{displaymath}
Q = `r m2l(qr_bases)`, \quad R = `r m2l(qr_r)`
\end{displaymath}

Thus, the basis for $C(X)$ will be the columns of the matrix $Q$.  

The Singular Value Decomposition of $X$ is:  
\begin{displaymath}
U = `r m2l(svd_5p1$u)`, \quad D = `r m2l(diag(svd_5p1$d))`,
\end{displaymath}

\begin{displaymath}
V = `r m2l(svd_5p1$v)`
\end{displaymath}


Thus, the basis for $C(X)$ will be the columns of the matrix $U$.  


\textbf{Part b}  

We can find the eigen (spectral) decomposition of $X'X$ by calculating the eigenvalues, $D^2$, and the eigenvectors, which will be the columns of $V$:  

```{r, echo = F, warning = F, message = F}
d_5p1 <- svd_5p1$d
v_5p1 <- svd_5p1$v
u_5p1 <- svd_5p1$u

xpx <- v_5p1 %*% diag(d_5p1^2) %*% t(v_5p1)
eigen_5p1 <- eigen(xpx)

xxp <- u_5p1 %*% diag(d_5p1^2) %*% t(u_5p1)
eigen2_5p1 <- eigen(xxp)
```


\begin{displaymath}
D^2 = `r m2l(matrix(d_5p1^2, nrow = 4))`, \quad V = `r m2l(v_5p1)`
\end{displaymath}


We can find the eigen (spectral) decomposition of $XX'$ by calculating the eigenvalues, $D^2$, and the eigenvectors, which will be the columns of $U$ (rather than $V$ as for $X'X$):  

\begin{displaymath}
D^2 = `r m2l(matrix(d_5p1^2, nrow = 4))`, \quad V = `r m2l(u_5p1)`
\end{displaymath}

\textbf{Part c}  


```{r, echo = F, warning = F, message = F}
u_1 <- u_5p1[,1]
v_1 <- v_5p1[,1]
d_1 <- d_5p1[1]

x_star1 <- d_1*u_1 %*% t(v_1)
```

The best $rank=1$ approximation to $X$ will be $X^{*1} = U_1 diag(d_1)V'_1$, where $U_1$ is a matrix with the first column of $U$, and $V_1$ is a matrix with the first column of $V$:  

\begin{displaymath}
X^{*1} = U_1 diag(d_1) V'_1 = `r m2l(matrix(u_1, nrow = 5))` \times `r m2l(matrix(d_1, nrow =1))` \times `r m2l(matrix(v_1, nrow = 1)) `
\end{displaymath}

\begin{displaymath}
= `r m2l(x_star1)`
\end{displaymath}


```{r, echo = F, warning = F, message = F}
u_2 <- u_5p1[,1:2]
v_2 <- v_5p1[,1:2]
d_2 <- diag(d_5p1[1:2])

x_star2 <- u_2 %*%d_2 %*% t(v_2)
```

The best $rank=2$ approximation to $X$ will be $X^{*2} = U_2 diag(d_1, d_2)V'_2$, where $U_2$ is a matrix with the first two columns of $U$, and $V_2$ is a matrix with the first two columns of $V$:  


\begin{displaymath}
X^{*2} = U_2 diag(d_1, d_2) V'_2 = `r m2l(u_2)` \times `r m2l(d_2)` \times `r m2l(t(v_2)) `
\end{displaymath}

\begin{displaymath}
= `r m2l(x_star2)`
\end{displaymath}



\textbf{Part d}  

```{r, echo = F, warning = F, message = F}
xtilde <- scale(prob5p1_mat, center = T, scale = F)
svd_xtilde <- svd(xtilde)
```


Note that 
\begin{displaymath}
\tilde{X} = `r m2l(xtilde)`
\end{displaymath}


The Singular Value Decomposition of $\tilde{X}$ is: 

\begin{displaymath}
\tilde{U} = `r m2l(svd_xtilde$u)`, \quad \tilde{D} = `r m2l(diag(svd_xtilde$d))`
\end{displaymath}

\begin{displaymath}
\tilde{V} = `r m2l(svd_xtilde$v)`
\end{displaymath}


Therefore, the principal component directions will be the columns of $\tilde{V}$, the principal components will be the inner products $z_j = \langle x_i, v_j\rangle$, the columns of: 

```{r, echo = F, warning = F, message = F}
ztilde_mat <- xtilde %*% svd_xtilde$v
```


\begin{displaymath}
z = `r m2l(ztilde_mat)`
\end{displaymath}

The "loadings" of the first principal component are the first column of the matrix $\tilde{V}$, 
 
\begin{displaymath}
\tilde{V}_1 = `r m2l(matrix(svd_xtilde$v[,1], nrow = 4))`
\end{displaymath}

\textbf{Part e}  

The best $rank = 1$ approximation to $\tilde{X}$ will be $\tilde{X}^{*1} = \tilde{U}_1 diag(\tilde{d}_1), \tilde{V}_1$, where $\tilde{U}_1$ is a matrix with first column of $\tilde{U}$ and $\tilde{V}_1$ is a matrix with the first column of $\tilde{V}$:  

```{r, echo = F, warning =F, message = F} 
utilde_1 <- svd_xtilde$u[,1]
vtilde_1 <- svd_xtilde$v[,1]
dtilde_1 <- svd_xtilde$d[1]

xtilde_star1 <- dtilde_1 * utilde_1 %*% t(vtilde_1)
```

\begin{displaymath}
\tilde{X}^{*1} = \tilde{U}_1 diag(\tilde{d}_1) \tilde{V}'_1 =  `r m2l(matrix(utilde_1, nrow = 5))` \times `r m2l(matrix(dtilde_1, nrow =1))` \times `r m2l(matrix(vtilde_1, nrow = 1)) `
\end{displaymath}

\begin{displaymath}
= `r m2l(xtilde_star1)`
\end{displaymath}

```{r, echo = F, warning =F, message = F} 
utilde_2 <- svd_xtilde$u[,1:2]
vtilde_2 <- svd_xtilde$v[,1:2]
dtilde_2 <- diag(svd_xtilde$d[1:2])

xtilde_star2 <-utilde_2 %*% dtilde_2 %*% t(vtilde_2)
```


The best $rank=2$ approximation to $\tilde{X}$ will be $\tilde{X}^{*2} = \tilde{U}_2 diag(\tilde{d}_1, \tilde{d}_2)\tilde{V}'_2$, where $\tilde{U}_2$ is a matrix with the first two columns of $\tilde{U}$, and $\tilde{V}_2$ is a matrix with the first two columns of $\tilde{V}$:  


\begin{displaymath}
\tilde{X}^{*2} = \tilde{U}_2 diag(\tilde{d}_1, \tilde{d}_2) \tilde{V}'_2 =  `r m2l(utilde_2)` \times `r m2l(dtilde_2)` \times `r m2l(t(vtilde_2)) `
\end{displaymath}

\begin{displaymath}
= `r m2l(xtilde_star2)`
\end{displaymath}


\textbf{Part f}  

Note that since here we are dealing with a sample covariance matrix, we have a symmetric non-negative definite matrix, so an eigen analysis will also yield all the information we need for a singular value decomposition, and thus we can easily find the best 1 and 2 component approximations.  


```{r, echo = F, warning = F, message = F}
tilde_covar <- 0.2*t(xtilde) %*% xtilde

tilde_covar_eigen <- eigen(tilde_covar)
tilde_covar_u <- tilde_covar_eigen$vectors
tilde_covar_d <- tilde_covar_eigen$values

tilde_covar_u1 <- tilde_covar_u[,1]
tilde_covar_d1 <- tilde_covar_d[1]
tilde_covar_star1 <- tilde_covar_u1 %*% matrix(tilde_covar_d1) %*% t(tilde_covar_u1)


tilde_covar_u2 <- tilde_covar_u[,1:2]
tilde_covar_d2 <- tilde_covar_d[1:2]
tilde_covar_star2 <- tilde_covar_u2 %*% diag(tilde_covar_d2) %*% t(tilde_covar_u2)
```

We begin with the matrix: 

\begin{displaymath}
\frac{1}{5}\tilde{X}' \tilde{X} = `r m2l(tilde_covar)`
\end{displaymath}


An eigen decomposition yields: 

\begin{displaymath}
\mbox{Eigenvalues} = D = `r m2l(matrix(tilde_covar_d, nrow = 4))`, \quad \mbox{Eigenvectors} = U = V = `r m2l(tilde_covar_u)`
\end{displaymath}

Thus, a best 1 component approximation will be of the form: 

\begin{displaymath}
\frac{1}{5}\tilde{X'}\tilde{X}^{*1} = \tilde{U}_1 diag(\tilde{d}_1) \tilde{V}'_1 =  `r m2l(matrix(tilde_covar_u1, nrow = 4))` \times `r m2l(matrix(tilde_covar_d1, nrow = 1))` \times `r m2l(matrix(tilde_covar_u1, nrow = 1))`
\end{displaymath}

\begin{displaymath}
= `r m2l(tilde_covar_star1)`
\end{displaymath}

A best 2 component approximation will be of the form: 

\begin{displaymath}
\frac{1}{5}\tilde{X'}\tilde{X}^{*2} = \tilde{U}_2 diag(\tilde{d}_1, \tilde{d}_2) \tilde{V}'_2 =  `r m2l(tilde_covar_u2)` \times `r m2l(diag(tilde_covar_d2))` \times `r m2l(t(tilde_covar_u2)) `
\end{displaymath}

\begin{displaymath}
= `r m2l(tilde_covar_star2)`
\end{displaymath}



\textbf{Part d - standardized}  

```{r, echo = F, warning = F, message = F}
x2tilde <- scale(prob5p1_mat, center = T, scale = T)
svd_x2tilde <- svd(x2tilde)
```


Note that 
\begin{displaymath}
\dbtilde{X} = `r m2l(x2tilde)`
\end{displaymath}


The Singular Value Decomposition of $\dbtilde{X}$ is: 

\begin{displaymath}
\dbtilde{U} = `r m2l(svd_x2tilde$u)`, \quad \dbtilde{D} = `r m2l(diag(svd_x2tilde$d))`
\end{displaymath}

\begin{displaymath}
\dbtilde{V} = `r m2l(svd_x2tilde$v)`
\end{displaymath}


Therefore, the principal component directions will be the columns of $\dbtilde{V}$, the principal components will be the inner products $z_j = \langle x_i, v_j\rangle$, the columns of: 

```{r, echo = F, warning = F, message = F}
z2tilde_mat <- x2tilde %*% svd_x2tilde$v
```


\begin{displaymath}
z = `r m2l(z2tilde_mat)`
\end{displaymath}

The "loadings" of the first principal component are the first column of the matrix $\dbtilde{V}$, 
 
\begin{displaymath}
\tilde{V}_1 = `r m2l(matrix(svd_x2tilde$v[,1], nrow = 4))`
\end{displaymath}


\textbf{Part e - standardized}  

The best $rank = 1$ approximation to $\dbtilde{X}$ will be $\dbtilde{X}^{*1} = \dbtilde{U}_1 diag(\dbtilde{d}_1), \dbtilde{V}_1$, where $\dbtilde{U}_1$ is a matrix with first column of $\dbtilde{U}$ and $\dbtilde{V}_1$ is a matrix with the first column of $\dbtilde{V}$:  

```{r, echo = F, warning =F, message = F} 
u2tilde_1 <- svd_x2tilde$u[,1]
v2tilde_1 <- svd_x2tilde$v[,1]
d2tilde_1 <- svd_x2tilde$d[1]

x2tilde_star1 <- d2tilde_1 * u2tilde_1 %*% t(v2tilde_1)
```

\begin{displaymath}
\dbtilde{X}^{*1} = \dbtilde{U}_1 diag(\dbtilde{d}_1) \dbtilde{V}'_1 =  `r m2l(matrix(u2tilde_1, nrow = 5))` \times `r m2l(matrix(d2tilde_1, nrow =1))` \times `r m2l(matrix(v2tilde_1, nrow = 1)) `
\end{displaymath}

\begin{displaymath}
= `r m2l(x2tilde_star1)`
\end{displaymath}

```{r, echo = F, warning =F, message = F} 
u2tilde_2 <- svd_x2tilde$u[,1:2]
v2tilde_2 <- svd_x2tilde$v[,1:2]
d2tilde_2 <- diag(svd_x2tilde$d[1:2])

x2tilde_star2 <-u2tilde_2 %*% d2tilde_2 %*% t(v2tilde_2)
```


The best $rank=2$ approximation to $\dbtilde{X}$ will be $\dbtilde{X}^{*2} = \dbtilde{U}_2 diag(\dbtilde{d}_1, \dbtilde{d}_2)\dbtilde{V}'_2$, where $\dbtilde{U}_2$ is a matrix with the first two columns of $\dbtilde{U}$, and $\dbtilde{V}_2$ is a matrix with the first two columns of $\dbtilde{V}$:  


\begin{displaymath}
\dbtilde{X}^{*2} = \dbtilde{U}_2 diag(\dbtilde{d}_1, \dbtilde{d}_2) \dbtilde{V}'_2 =  `r m2l(u2tilde_2)` \times `r m2l(d2tilde_2)` \times `r m2l(t(v2tilde_2)) `
\end{displaymath}

\begin{displaymath}
= `r m2l(x2tilde_star2)`
\end{displaymath}


\textbf{Part f - standardized}  


Note that since here we are dealing with a sample covariance matrix, we have a symmetric non-negative definite matrix, so an eigen analysis will also yield all the information we need for a singular value decomposition, and thus we can easily find the best 1 and 2 component approximations.  


```{r, echo = F, warning = F, message = F}
tilde2_covar <- 0.2*t(x2tilde) %*% x2tilde

tilde2_covar_eigen <- eigen(tilde2_covar)
tilde2_covar_u <- tilde2_covar_eigen$vectors
tilde2_covar_d <- tilde2_covar_eigen$values

tilde2_covar_u1 <- tilde2_covar_u[,1]
tilde2_covar_d1 <- tilde2_covar_d[1]
tilde2_covar_star1 <- tilde2_covar_u1 %*% matrix(tilde2_covar_d1) %*% t(tilde2_covar_u1)


tilde2_covar_u2 <- tilde2_covar_u[,1:2]
tilde2_covar_d2 <- tilde2_covar_d[1:2]
tilde2_covar_star2 <- tilde2_covar_u2 %*% diag(tilde2_covar_d2) %*% t(tilde2_covar_u2)
```

We begin with the matrix: 

\begin{displaymath}
\frac{1}{5}\dbtilde{X}' \dbtilde{X} = `r m2l(tilde2_covar)`
\end{displaymath}


An eigen decomposition yields: 

\begin{displaymath}
\mbox{Eigenvalues} = D = `r m2l(matrix(tilde2_covar_d, nrow = 4))`, \quad \mbox{Eigenvectors} = U = V = `r m2l(tilde2_covar_u)`
\end{displaymath}

Thus, a best 1 component approximation will be of the form: 

\begin{displaymath}
\frac{1}{5}\dbtilde{X'}\dbtilde{X}^{*1} = \dbtilde{U}_1 diag(\dbtilde{d}_1) \dbtilde{V}'_1 =  `r m2l(matrix(tilde2_covar_u1, nrow = 4))` \times `r m2l(matrix(tilde2_covar_d1, nrow = 1))` \times `r m2l(matrix(tilde2_covar_u1, nrow = 1))`
\end{displaymath}

\begin{displaymath}
= `r m2l(tilde2_covar_star1)`
\end{displaymath}

A best 2 component approximation will be of the form: 

\begin{displaymath}
\frac{1}{5}\dbtilde{X'}\dbtilde{X}^{*2} = \dbtilde{U}_2 diag(\dbtilde{d}_1, \dbtilde{d}_2) \dbtilde{V}'_2 =  `r m2l(tilde2_covar_u2)` \times `r m2l(diag(tilde2_covar_d2))` \times `r m2l(t(tilde2_covar_u2)) `
\end{displaymath}

\begin{displaymath}
= `r m2l(tilde2_covar_star2)`
\end{displaymath}











## Problem 2  

### Part a  
A scatterplot of the data and the fitted values from the OLS model are shown below.

```{r, eval = TRUE, echo = FALSE, warning=FALSE, message=FALSE}
d3.4 <- read.csv(file = "data/data_3pt4.csv")

haar_basis <- function(x,m)
{
  ## define the endpoints of the interval on which we desire the basis
  a <- min(x)
  b <- max(x)
  
  ## transform the x-values to be on (0,1)
  xtrans <- (x - a)/(b - a)
  
  ## create father function
  father <- function(x)
  {
    return(1 * ( (0 <= x) & (x <= 1) ) )
  }
  
  ## create mother function
  mother <- function(x)
  {
    return(father(2*x) - father(2*x - 1))
  }
  
  ## psi is a list where each element is a matrix corresponding to the basis functions corresponding to a specific m
  ## X is the design matrix
  psi <- list()
  X <- cbind(father(xtrans), mother(xtrans))
  for(i in 1:m)
  {
    psi[[i]] <- matrix(nrow = length(x), ncol = length(0:(2^i - 1)))
    for(j in 0:(2^i - 1))
    {
      psi[[i]][,j + 1] <- sqrt(2^i) * mother(2^i * (xtrans - j/(2^i)))
    }
    X <- cbind(X, psi[[i]])
  }
  return(X)
}

## create design matrix
X <- haar_basis(x = d3.4$x, m = 3)

## find beta hat
mod_haar <- lm(formula = d3.4$y ~ X - 1)

## plot data and wavelet predictions
d3.4$fit <- mod_haar$fitted.values
plot(d3.4$x, y = d3.4$y, xlab = "x", ylab = "y")
points(x = d3.4$x[order(d3.4$x)], y = d3.4$fit[order(d3.4$x)], type = "l")

```


### Part b  

The solid line in the plot below gives (approximately) the fitted values when there are 2 nonzero coefficients, the dashed line are fitted values when there are 4 nonzero coefficients, and the dotted line are fitted values for 8 nonzero coefficients. 

```{r, echo = FALSE ,eval = TRUE, message=FALSE, warning=FALSE}
library(glmnet)

lasso_fits <- glmnet(x = X, y = d3.4$y, family = "gaussian", lambda = seq(from = 0, to = 0.5, by = 0.001))

nonzero_counts <- numeric(length = ncol(lasso_fits$beta))
nonzero_counts <- apply(X = lasso_fits$beta, MARGIN = 2, FUN = function(x){sum(1*(x!=0))})

ind <- numeric()
ind[1] <- max(which(x = nonzero_counts == 2))
ind[2] <- max(which(x = nonzero_counts == 4))
ind[3] <- max(which(x = nonzero_counts == 8))

beta1 <- lasso_fits$beta[,ind[1]]
beta2 <- lasso_fits$beta[,ind[2]]
beta3 <- lasso_fits$beta[,ind[3]]

## plot data and wavelet predictions
plot(d3.4$x, y = d3.4$y, xlab = "x", ylab = "y")
points(x = d3.4$x[order(d3.4$x)], y = (X %*% beta1)[order(d3.4$x)], type = "l", lty = 1, lwd = 2)
points(x = d3.4$x[order(d3.4$x)], y = (X %*% beta2)[order(d3.4$x)], type = "l", lty = 2, lwd = 2)
points(x = d3.4$x[order(d3.4$x)], y = (X %*% beta3)[order(d3.4$x)], type = "l", lty = 3, lwd = 2)


```

## Problem 3  

```{r, eval = TRUE, echo = FALSE, warning=FALSE, message=FALSE}
library(splines)
d3.4 <- read.csv(file = "data/data_3pt4.csv")
xsi <- c(0.1, 0.3, 0.5, 0.7, 0.9)

Xspline <- ns(x = d3.4$x, knots = xsi, intercept = TRUE, Boundary.knots = c(0,1))

mod_spline <- lm(formula = d3.4$y ~ Xspline - 1)

plot(d3.4$x, d3.4$y, xlab = "x", ylab = "y")
points(d3.4$x[order(d3.4$x)], mod_spline$fitted.values[order(d3.4$x)], type = "l")

```


## Problem 4

### Part a   

```{r, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE}
x <- seq(from = 0, to = 1, by = 0.1)
y <- c(0,2.5,2,0.5,0,-0.5,0,1.5,3.5,4.5,3.5)

cubic_natspline_basis <- function(x, knots) 
{
  K <- length(knots)
  H <- matrix(nrow = length(x), ncol = length(knots))
  H[,1] <- 1
  H[,2] <- x
  for(i in 1:(K - 2))
  {
    H[,i + 2] <- (x - knots[i])^3 * (x - knots[i] > 0) + 
      (knots[K] - knots[i])/(knots[K] - knots[K - 1]) * (x - knots[K - 1])^3 * (x - knots[K -1] > 0) + 
      (knots[K - 1] - knots[i])/(knots[K] - knots[K - 1]) * (x - knots[K])^3 * (x - knots[K] > 0)
  }
  return(H)
  
}

H <- cubic_natspline_basis(x = x, knots = x)

print("Matrix H")
H

O <- matrix(nrow = length(x), ncol = length(x), data = 0)
for(i in 3:(length(x)))
{
  for(j in 3:length(x))
  {
    if(i == j)
    {
      O[i,j] <- 12 * (x[length(x) - 1] - x[i])^2 * (x[length(x)] - x[i])
    }
    if(i < j)
    {
      O[i,j] <- 6 * (x[length(x) - 1] - x[j])^2 * (2 * x[length(x) - 1] + x[j] - 3*x[i]) + 
        12*(x[length(x) - 1] - x[j]) * (x[length(x) - 1] - x[i]) * (x[length(x)] - x[length(x) - 1])
    }
    if(i > j)
    {
      O[i,j] <- 6 * (x[length(x) - 1] - x[i])^2 * (2 * x[length(x) - 1] + x[i] - 3*x[j]) + 
        12*(x[length(x) - 1] - x[i]) * (x[length(x) - 1] - x[j]) * (x[length(x)] - x[length(x) - 1])
    }
  }
}

print("Matrix Omega")
O

K <- solve(t(H)) %*% O %*% solve(H)

print("Matrix K")
K
```


### Part b  

```{r, eval=TRUE, echo = FALSE, warning=FALSE, message=FALSE}
ek <- eigen(x = K)
# ek$values
# ek$vectors

plot(x = 1:11, ek$vectors[,1], type = "l", xlab = "row index", ylab = "Eigenvector coordinate values")
points(x = 1:11, ek$vectors[,1])

points(x = 1:11, ek$vectors[,2], pch = 2)
points(x = 1:11, ek$vectors[,2], type = "l")

points(x = 1:11, ek$vectors[,3], pch = 3)
points(x = 1:11, ek$vectors[,3], type = "l")

points(x = 1:11, ek$vectors[,4], pch = 4)
points(x = 1:11, ek$vectors[,4], type = "l")

points(x = 1:11, ek$vectors[,5], pch = 5)
points(x = 1:11, ek$vectors[,5], type = "l")

points(x = 1:11, ek$vectors[,6], pch = 6)
points(x = 1:11, ek$vectors[,6], type = "l")

points(x = 1:11, ek$vectors[,7], pch = 7)
points(x = 1:11, ek$vectors[,7], type = "l")

points(x = 1:11, ek$vectors[,8], pch = 8)
points(x = 1:11, ek$vectors[,8], type = "l")

points(x = 1:11, ek$vectors[,9], pch = 9)
points(x = 1:11, ek$vectors[,9], type = "l")

points(x = 1:11, ek$vectors[,10], pch = 10)
points(x = 1:11, ek$vectors[,10], type = "l")

points(x = 1:11, ek$vectors[,11], pch = 11)
points(x = 1:11, ek$vectors[,11], type = "l")
```

### Part c  

```{r, eval = TRUE, echo = FALSE, warning=FALSE, message=FALSE}
lambda <- seq(from = 0.001, to = 2, by = 0.01)
eff_df <- numeric(length = length(lambda))
for(i in 1:length(lambda))
{
  eff_df[i] <- sum(diag(H %*% solve(a = t(H) %*% H + lambda[i] * O) %*% t(H)))
}

plot(x = lambda, y = eff_df, xlab = "lambda", ylab = "Effective DF")

sum(diag(H %*% solve(a = t(H) %*% H + 0.0012 * O) %*% t(H))) ## df approximately = 5
sum(diag(H %*% solve(a = t(H) %*% H + 0.0059 * O) %*% t(H))) ## df approximately = 4
sum(diag(H %*% solve(a = t(H) %*% H + 0.0375 * O) %*% t(H))) ## df approximately = 3
sum(diag(H %*% solve(a = t(H) %*% H + 0.12 * O) %*% t(H))) ## df approximately = 2.5

```

The sequence of effective degrees of freedom $2.5, 3, 4, 5$ approximately corresponds to the sequence of $\lambda$ values $0.0012, 0.0059, 0.0375, 0.12$.

## Problem 5  

### Part a  

In order to compute and plot effective degrees of freedom, we need to compute the matrix 

$$
L_{\lambda} = 
\begin{bmatrix}
l'(x_1) \\
\vdots \\
l'(x_{11}) \\
\end{bmatrix}
$$
where $l'(x_i) = (1, x_i) (\mathbf{B}' \mathbf{W}(x_i)\mathbf{B})^{-1} \mathbf{B}' \mathbf{W}(x_i)$, where 

$$
\mathbf{B} = 
\begin{bmatrix}
1 & x_1 \\
1 & x_2 \\
\vdots & \vdots \\
1 & x_{11} \\
\end{bmatrix}, \quad
\mathbf{W} = diag(K_{\lambda}(x_i, x_1), \dots, K_{\lambda}(x_i, x_N))
$$

A plot can be seen in \autoref{prob5}. The corresponding table for specific degrees of freedom can be seen in \autoref{prob5table}. 


```{r, echo = F, warning = F, message = F, fig.cap ="\\label{prob5} A plot of effective degrees of freedom as a function of Lambda values for Problem 5.", fig.height = 3}
prob5_xs <- c(0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1)

prob5_bmat <- matrix(c(rep(1, 11), prob5_xs), nrow = 11, byrow = F)

prob5_kernel <- function(lambda, x_0, x){
  val <- (x_0 - x)/lambda
  pnorm(val)
}

calc_wmat <- function(lambda, x_0, x_vec){
  mat <- c()
  for(i in 1:length(x_vec)){
    kern <- prob5_kernel(lambda = lambda, x_0 = x_0, x = x_vec[i])
    mat <- c(mat, kern)
  }
  return(diag(mat))
}

calc_littlel <- function(lambda, x_0, x_vec){
  start <- c(1, x_0)
  littlel <- start %*% solve(t(prob5_bmat) %*% calc_wmat(lambda, x_0, x_vec) %*% prob5_bmat) %*% t(prob5_bmat) %*% calc_wmat(lambda, x_0, x_vec)
  return(littlel)
}

eff_df <- function(lambda){
  l_lambda <- matrix(NA, nrow = 11, ncol = 11)
  for(i in 1:length(prob5_xs)){
    l_lambda[i,] <- calc_littlel(lambda, x_0 = prob5_xs[i], x_vec = prob5_xs)
  }
  trace <- sum(diag(l_lambda))
  return(trace)
}

lambdas <- seq(.02, 10, length.out = 1000)

prob5_df <- data.frame(lambda = lambdas) %>% mutate(eff_df = purrr::map_dbl(lambda, .f = function(lambdas){
  eff_df(lambdas)
}))


df_5 <- prob5_df$lambda[which.min(abs(prob5_df$eff_df - 5))]
df_4 <- prob5_df$lambda[which.min(abs(prob5_df$eff_df - 4))]
df_3 <- prob5_df$lambda[which.min(abs(prob5_df$eff_df - 3))]
df_2.5 <- prob5_df$lambda[which.min(abs(prob5_df$eff_df - 2.5))]
lambda_4 <- 0.0999

prob5_df %>% ggplot() + 
  geom_line(aes(x = log(lambda), y = eff_df)) + theme_bw() + 
  labs(x = "Lambda value (log scale)", y = "Effective Degrees of Freedom")
```

\begin{table}[h]
\centering
\caption{Values of $\lambda$ for effective degrees of freedom.}
\begin{tabular}{lc}
\textbf{Effective DF} & $\lambda$ value \\ \hline
2.5 & `r df_2.5` \\ \hline
3 & `r df_3`\\ \hline
4 & `r df_4`\\ \hline
5 & `r df_5`\\ \hline
\label{prob5table}
\end{tabular}
\end{table}


\newpage

### Part b  

First, note that we are working with $\lambda =$ `r lambda_4`, and our $L_{\lambda}$ matrix is:
```{r, echo = F, warning = F, message = F}
calc_lambda <- function(lambda){
  l_lambda <- matrix(NA, nrow = 11, ncol = 11)
  for(i in 1:length(prob5_xs)){
    l_lambda[i,] <- calc_littlel(lambda, x_0 = prob5_xs[i], x_vec = prob5_xs)
  }
  return(l_lambda)
}

L_lambda <- calc_lambda(lambda = lambda_4)

## waiting on Question 4 part C matrix for this! 
round(L_lambda, 2)
```

And our $S_{\lambda}$ matrix is:  

```{r, echo = F, warning = F, message = F}

S_lambda <- H %*% solve(a = t(H) %*% H + 0.0059 * O) %*% t(H)
round(S_lambda, 2)

#\begin{displaymath}
#`r m2l(round(L_lambda,2))`
#\end{displaymath}
```


Thus, the 11x11 matrix difference is:  

```{r, echo = F, warning = F, message = F}
round(S_lambda - L_lambda, 2)
```

The described plot can be seen in \autoref{prob5b}.  

```{r, echo = F, warning = F, message = F, fig.cap = "\\label{prob5b} Plot comparing the first, third, and fifth columns of the S_lambda and L_lambda matrices for 4 effective degrees of freedom. This if for problem 5 part b.", fig.height = 3}
matrix_comp <- data.frame(S_col1 = S_lambda[,1], S_col3 = S_lambda[,3], S_col5 = S_lambda[,5],
                          L_col1 = L_lambda[,1], L_col3 = L_lambda[,3], L_col5 = L_lambda[,5])

matrix_comp %>% 
  mutate(x = 1:11) %>% 
  gather(1:6, key = "Matrix_Column", value = "Value") %>%
  separate(Matrix_Column, c("Matrix", "Column"), sep = "_") %>%
  ggplot() + geom_line(aes(x = x, y = Value, color = factor(Column), linetype = factor(Matrix))) + 
  geom_point(aes(x = x, y = Value, color = factor(Column)), pch = 1) + 
  theme_bw() + 
  labs(title = "Columns 1, 3, and 5 For S and L matrices.")

```



## Problem 6  

```{r, echo = F, warning = F, message = F}

#p6_data <- read_csv("data/Problem3-4Data.csv")
p6_data <- data.frame(x = c(0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1), y = c(0, 1.5, 2, .5, 0, -.5, 0, 1.5, 3.5, 4.5, 3.5))

```

### Part a  

The plots for this problem can be seen in \autoref{prob6pa}.  

```{r prob6pa, echo = F, warning = F, message = F, fig.cap ="\\label{prob6pa} Plots for effective degrees of freedom 5 (top) and 9 (bottom) for Problem 6(a). "}
spline5 <- smooth.spline(x = p6_data$x, y = p6_data$y, df = 5)
spline5 <- data.frame(x = spline5$x, spline5 = spline5$y)
loess5 <- loess(y~x, data = p6_data, enp.target = 5, degree = 1)


p6_data <- p6_data %>% mutate(loess5 = loess5$fitted) %>% arrange(x) %>% full_join(spline5)


plot_5df <- p6_data %>% ggplot() + geom_point(aes(x = x, y = y)) + 
  geom_line(aes(x = x, y = spline5, color = "spline")) + 
  geom_line(aes(x = x, y = loess5, color = "loess")) + 
  scale_color_manual(name = "Fitting method", values = c("cyan", "coral"), 
                     breaks = c("spline", "loess"), aesthetics = c("colour")) + 
  theme_bw() + 
  labs(x = "X", y = "Y", title = "Approximately 5 DF")



spline9 <- smooth.spline(x = p6_data$x, y = p6_data$y, df = 9)
spline9 <- data.frame(x = spline9$x, spline9 = spline9$y)
loess9 <- loess(y~x, data = p6_data, enp.target = 9, degree = 1)


p6_data <- p6_data %>% mutate(loess9 = loess9$fitted) %>% arrange(x) %>% full_join(spline9)


plot_9df <- p6_data %>% ggplot() + geom_point(aes(x = x, y = y)) + 
  geom_line(aes(x = x, y = spline9, color = "spline")) + 
  geom_line(aes(x = x, y = loess9, color = "loess")) + 
  scale_color_manual(name = "Fitting method", values = c("cyan", "coral"), 
                     breaks = c("spline", "loess"), aesthetics = c("colour")) + 
  theme_bw() + 
  labs(x = "X", y = "Y", title = "Approximately 9 DF")


grid.arrange(plot_5df, plot_9df, ncol = 1)

```


## Problem 7  

A scatterplot matrix for the five different methods and the response can be seen in \autoref{wine-scat}.  


```{r, echo = F, warning = F, message = F, fig.cap = "\\label{wine-scat} A scatterplot matrix for the five different prediction methods in Problem 7."}
wines <- read_delim("data/winequality-white.csv", delim = ";")


# k nearest neighbors
wineControl <- trainControl(method = "cv", number = 10)

wine_knn <- caret::train(form = quality~., data = wines, 
                       method = "knn", preProcess = c("center", "scale"), 
                       trControl = wineControl, 
                       tuneLength = 20)

preds_knn <- predict(wine_knn, newdata = wines)


# elastic net
wine_enet <- caret::train(form = quality~., data = wines, 
                       method = "enet", preProcess = c("center", "scale"), 
                       trControl = wineControl, 
                       tuneLength = 20)

preds_enet <- predict(wine_enet, newdata = wines)

# principal components regression
wine_pcr <- caret::train(form = quality~., data = wines, 
                       method = "pcr", preProcess = c("center", "scale"), 
                       trControl = wineControl, 
                       tuneLength = 20)

preds_pcr <- predict(wine_pcr, newdata = wines)


# partial least squares 
wine_pls <- caret::train(form = quality~., data = wines, 
                       method = "pls", preProcess = c("center", "scale"), 
                       trControl = wineControl, 
                       tuneLength = 20)

preds_pls <- predict(wine_pls, newdata = wines)


# MARS
#wine_mars <- caret::train(form = quality~., data = wines, 
#                       method = "earth", preProcess = c("center", "scale"), 
#                       trControl = wineControl, 
#                       tuneLength = 20)

#preds_mars <- predict(wine_mars, newdata = wines)
#saveRDS(preds_mars, "data/preds_mars")

## This takes a long time to re-render, so I saved and re-read in the predictions to save computing whenever we render the document.
preds_mars <- readRDS("data/preds_mars")

wine_preds <- data.frame(quality = wines$quality, knn = preds_knn, 
                         enet = preds_enet, pcr = preds_pcr,
                         pls = preds_pls, mars = preds_mars[,1])

wine_cor_mat <- cor(wine_preds)
pairs(wine_preds)
```

A correlation matrix can be seen below:  
```{r, echo = F, warning = F, message = F}
wine_cor_mat <- cor(wine_preds)
wine_cor_mat
```

## Problem 8

### Part a  

Consider the Gram-Schmidt process using the \(L_2\) inner product
\(\int{f(x)g(x)dx}\) on functions \(f_c(x) = e^{ -\frac{(c - x)^2}{2}}\)
for \(c = x_1, x_2, x_3\). Define

\begin{align} 
u_1(x) &= f_{x_1}(x) / \left[ \int{f_{x_1}^2(x) dx} \right] ^{1/2} \\
&= e^ { -\frac{(x_1 - x)^2}{2}} / \left[ \int{ e^{ -(x_1 - x)^2 } dx} \right] ^{1/2} \\
&= \pi^{1/4} e^{ -\frac{(x_1 - x)^2}{2}}.
\end{align}

Next, define

\begin{align}
u_2(x) &= \frac{f_{x_2}(x) - \int{f_{x_2}(x)u_1(x) dx} u_1(x)}{\left[ \int{ (f_{x_2}(x) - \int{f_{x_2}(x)u_1(x) dx} u_1(x))^2 } \right]^{1/2}} \\
&= \frac{f_{x_2}(x) - (\pi^{1/4}e^{\frac{1}{4}(x_1 + x_2)^2 - 2 x_1 x_2}) \int{e^{-\frac{1}{2 * 1/2} (x - 1/2(x_1 + x_2)^2)} dx} * u_1(x)}{\left[ \int{ (f_{x_2}(x) - (\pi^{1/4}e^{\frac{1}{4}(x_1 + x_2)^2 - 2 x_1 x_2}) \int{e^{-\frac{1}{2 * 1/2} (x - 1/2(x_1 + x_2)^2)} dx} * u_1(x))^2 } \right]^{1/2}} \\
&= \frac{f_{x_2}(x) - (\pi^{1/4}e^{\frac{1}{4}(x_1 + x_2)^2 - 2 x_1 x_2}) \pi^{1/2} \pi^{1/4} e^{ -\frac{(x_1 - x)^2}{2}}}{\left[ \int{ (f_{x_2}(x) - (\pi^{1/4}e^{\frac{1}{4}(x_1 + x_2)^2 - 2 x_1 x_2}) \pi^{1/2} \pi^{1/4} e^{ -\frac{(x_1 - x)^2}{2}})^2 } \right]^{1/2}} \\
&= \frac{e^{-(x_2 - x)^2}/2}{\pi^{1/2} - 2q(x_1, x_2) \sqrt{2\pi} + q(x_1, x_2)},
\end{align}

where \(q(c_1, c_2) = \pi e^{\frac{1}{4} (c_1 + c_2)^2 - 2c_1c_2}\).

Following the same procedure for the third function \(u_3(x)\), we get

\begin{align}
u_3(x) &= \frac{f_{x_3}(x) - \int{f_{x_3}(x)u_2(x)dx} * u_2(x) - \int{f_{x_3}(x)u_1(x)dx} * u_1(x)}{\left[ \int{(f_{x_3}(x) - \int{f_{x_3}(x)u_2(x)dx} * u_2(x) - \int{f_{x_3}(x)u_1(x)dx} * u_1(x))^2} \right]^{1/2}} \\
&= \frac{f_{x_3}(x) - \int{f_{x_3}(x)u_2(x)dx} * u_2(x) - \int{f_{x_3}(x)u_1(x)dx} * u_1(x)} {v(x_1,x_2,x_3)} \\
&= \frac{f_{x_3}(x) - \frac{\pi^{-1/2}q(x_2,x_3) e^{-(x_2 - x)^2/2} }{\pi^{1/2} + 2 q(x_2,x_3)\sqrt{2\pi} + q(x_2,x_3)} - 1(x_1,x_3) e^{-(x_1 - x)^2 / 2} } {v(x_1,x_2,x_3)}, 
\end{align}

where $v(x_1,x_2,x_3) = \pi^{1/2} + \left( \frac{q(x_2,x_3)}{\pi^{1/2} + 2 q(x_2,x_3) \sqrt{2\pi} + q(x_2,x_3)} \right)^2 + q(x_1, x_3)^2 \pi^{1/2} - 2 \frac{\pi^{-1/2} q(x_2,x_3)q(x_1,x_3)}{\pi^{1/2} + 2q(x_2,x_3)\sqrt{2\pi} + q(x_2,x_3)} \pi^{-1/2}q(x_1,x_2) -2\frac{\pi^{-1/2} q(x_2,x_3)}{\pi^{1/2} + 2q(x_2,x_3)\sqrt{2\pi} + q(x_2,x_3)}\pi^{-1/2}q(x_2,x_3) - 2q(x_1,x_3) \pi^{-1/2}q(x_1,x_3)$.

If we consider the kernel inner product, the inner product expressions are much nicer. Using the kernel inner product, we wind up with 

\begin{align}
u_1(x) &= f_{x_1}(x) \\
u_2(x) &= \frac{f_{x_2}(x) - K(x_1,x_2)u_1(x)}{|1 - K(x_1,x_2)|} \\
u_3(x) &= \frac{f_{x_3}(x) - \langle f_{x_3}(x), u_2(x) \rangle u_2(x) - \langle f_{x_3}(x), u_1(x) \rangle u_1(x) }{\langle f_{x_3}(x) - \langle f_{x_3}(x), u_2(x) \rangle u_2(x) - \langle f_{x_3}(x), u_1(x) \rangle u_1(x) , f_{x_3}(x) - \langle f_{x_3}(x), u_2(x) \rangle u_2(x) - \langle f_{x_3}(x), u_1(x) \rangle u_1(x)  \rangle}.
\end{align}

These are not the same functions, as the inner products are different. However, the inner products essentially just control constants that amount to centering and scaling factors. 

### Part b  

```{r, eval = TRUE, echo = FALSE}
cov_fun_sqrd_exp <- function(x1, x2, cov_par)
{
  sigma <- cov_par$sigma
  l <- cov_par$l
  
  return(sigma^2 * exp(-1/(2*l^2) * (x1 - x2)^2))
}

make_cov_mat <- function(x, x_pred, cov_fun, cov_par, tau = 0)
{
  xfull <- rbind(x,x_pred)
  temp <- matrix(nrow = nrow(xfull), ncol = nrow(xfull))
  for(i in 1:nrow(xfull))
  {
    for(j in 1:nrow(xfull))
    {
      temp[i,j] <- cov_fun(xfull[i,], xfull[j,], cov_par) + 1*(i==j)*tau^2
    }
  }
  return(temp)
}

G_uncentered <- make_cov_mat(x = as.matrix(x, ncol = 1), x_pred = numeric(), cov_fun = cov_fun_sqrd_exp, cov_par = list("sigma" = 1, "l" = 1))

x <- seq(from = 0, to = 1, by = 0.1)
y <- c(0,2.5,2,0.5,0,-0.5,0,1.5,3.5,4.5,3.5)
G <- matrix(nrow = length(x), ncol = length(x))
for(i in 1:nrow(G))
{
  for(j in 1:ncol(G))
  {
    G[i,j] <- cov_fun_sqrd_exp(x1 = x[i], x2 = x[j], cov_par = list("sigma" = 1, "l" = 1)) - 
      (1/11) * sum(G_uncentered[i,]) - (1/11) * sum(G_uncentered[j,]) + (1/11^2) * sum(G_uncentered)
      
  }
}

print("Centered Gram Matrix")
G
```
### Part c  

```{r, eval = TRUE, echo = FALSE}
eG <- eigen(x = G)
xseq <- seq(from = 0, to = 1, by = 0.01)

sfun <- matrix(nrow = 11, ncol = length(xseq))
center_fun <- function(x1, x2)
{
  temp <- numeric(length(x2))
  for(i in 1:length(temp))
  {
    temp[i] <- cov_fun_sqrd_exp(x1 = x1, x2 = x2[i], cov_par = list("sigma" = 1, "l" = 1))
  }
  return(
    sum(temp)
  )
}

for(i in 1:nrow(sfun))
{
  for(j in 1:ncol(sfun))
  {
    sfun[i,j] <- cov_fun_sqrd_exp(x1 = x[i], x2 = xseq[j],cov_par = list("sigma" = 1, "l" = 1)) - 
      (1/11) * center_fun(x1 = xseq[j], x2 = x)
  }
}


plot(x = xseq, y = t(sfun) %*% eG$vectors[,1], type = "l", xlab = "x", ylab = "Kernel Principal Component Functions", lty = 1)
# points(x = xseq, y = sfun[1,])

points(x = xseq, y = t(sfun) %*% eG$vectors[,2], type = "l",  lty = 2)
# points(x = xseq, y = sfun[2,])

points(x = xseq, y = t(sfun) %*% eG$vectors[,3], type = "l",  lty = 3)
# points(x = xseq, y = sfun[3,])

points(x = xseq, y = t(sfun) %*% eG$vectors[,4], type = "l",  lty = 4)
# points(x = xseq, y = sfun[4,])

points(x = xseq, y = t(sfun) %*% eG$vectors[,5], type = "l",  lty = 5)
# points(x = xseq, y = sfun[5,])

points(x = xseq, y = t(sfun) %*% eG$vectors[,6], type = "l",  lty = 6)
# points(x = xseq, y = sfun[6,])

points(x = xseq, y = t(sfun) %*% eG$vectors[,7], type = "l", lty = 1)
points(x = xseq, y = t(sfun) %*% eG$vectors[,7])

points(x = xseq, y = t(sfun) %*% eG$vectors[,8], type = "l", lty = 2)
points(x = xseq, y = t(sfun) %*% eG$vectors[,8])

points(x = xseq, y = t(sfun) %*% eG$vectors[,9], type = "l", lty = 3)
points(x = xseq, y = t(sfun) %*% eG$vectors[,9])

points(x = xseq, y = t(sfun) %*% eG$vectors[,10], type = "l", lty = 4)
points(x = xseq, y = t(sfun) %*% eG$vectors[,10])

points(x = xseq, y = t(sfun) %*% eG$vectors[,11], type = "l", lty = 5)
points(x = xseq, y = t(sfun) %*% eG$vectors[,11])
```

As the eigenvalues decrease, the kernal principal components get closer to the zero function.

### Part d  

This was Nate's problem which he was unable to complete.




