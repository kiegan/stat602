---
title: "Stat 602 Homework 2"
author: "Kiegan Rice and Nate Garton"
date: "Due 3/4/2019"
output: 
  pdf_document: 
    fig_caption: yes
header-includes: \usepackage{float, mathtools}
---

```{r global_options, echo = F, include = FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```

```{r packages, echo = F, warning = F, message = F}
library(tidyverse)
library(gridExtra)
library(knitr)
```

## Problem 1  

### Problem 4.3  

```{r prob4p3, echo = F, warning = F, message = F}
prob4p3_data <- read_csv("data/AmesHousingData.csv")
prob4p3_data <- prob4p3_data %>% select(Price, Size, Fireplace, `Bsmt Bath`, Land)

```

### Problem 4.4  

\textbf{Part a}
```{r, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE}
library(cvTools)
library(class)
library(caret)
glass <- read.table(file = "data/glass_data_4pt4.txt", sep = ",", strip.white = TRUE)[,-1]
colnames(glass) <- c("RI","Na", "Mg", "Al", "Si","K","Ca","Ba","Fe","type")

glass12 <- subset(x = glass, subset = type %in% c("1","2"))
glass12$type <- as.factor(glass12$type)
# glass12[,1:(ncol(glass12) - 1)] <- scale(x = glass12[,1:(ncol(glass12) - 1)], center = TRUE, scale = TRUE)

k <- seq(from = 1, to = 20, by = 1)
tune_par <- data.frame("k" = k)
fitcontrol <- trainControl(method = "cv", number = 10)
knn_results <- caret::train(form = type ~ ., 
                            data = glass12, 
                            method = "knn", 
                            preProcess = c("center","scale"), 
                            tuneGrid = tune_par,
                            trControl = fitcontrol)

knn_results
```

Based on 10-fold cross-validation, the accuracy is estimated to be highest when $k = 4$.

\textbf{Part b}

The classification rule $\hat{y} = 2 I\left[t(x) \geq k/2\right] + I\left[t(x) < k/2\right]$ is equivalent to $\hat{y} = 2 I\left[t(x)/k \geq 1/2\right] + I\left[t(x)/k < 1/2\right]$. Here, $t(x)/k$ is an estimate of $E\left[ \text{type} = 2|x \right] = P(\text{type} = 2|x)$. Thus, this is directly an estimate of a posterior class probability which accounts for the prior class probabilities. This means that no modification of kNN needs to be made to account for differing prior probabilities.

### Problem 5.1  

```{r prob5p1, echo = F, warning = F, message = F}
prob5p1_mat <- matrix(c(2, 4, 7, 2, 
                        4, 3, 5, 5, 
                        3, 4, 6, 1, 
                        5, 2, 4, 2, 
                        1, 3, 4, 4), nrow = 5, byrow = T)

#Note: Most of this is on 602 HW2 from 2015 (problem 6)  

m2l <- function(matr) {

    printmrow <- function(x) {

        ret <- paste(paste(round(x, 2),collapse = " & "),"\\\\")
        sprintf(ret)
    }

    out <- apply(matr,1,printmrow)
    out2 <- paste("\\begin{bmatrix}",paste(out,collapse=' '),"\\end{bmatrix}")
    return(out2)
}


```

The following are decompositions using the matrix
\begin{displaymath}
X = `r m2l(prob5p1_mat)`
\end{displaymath}

\textbf{Part a}  

```{r, echo = F, warning = F, message = F, results='asis'}
qr_5p1 <- qr(prob5p1_mat)
qr_bases <- qr.Q(qr_5p1)
qr_r <- qr.R(qr_5p1)


svd_5p1 <- svd(prob5p1_mat)
```

The QR decomposition of $X$ is: 

\begin{displaymath}
Q = `r m2l(qr_bases)`, \quad R = `r m2l(qr_r)`
\end{displaymath}

Thus, the basis for $C(X)$ will be the columns of the matrix $Q$.  

The Singular Value Decomposition of $X$ is:  
\begin{displaymath}
U = `r m2l(svd_5p1$u)`, \quad D = `r m2l(diag(svd_5p1$d))`,
\end{displaymath}

\begin{displaymath}
V = `r m2l(svd_5p1$v)`
\end{displaymath}


Thus, the basis for $C(X)$ will be the columns of the matrix $U$.  


\textbf{Part b}  

We can find the eigen (spectral) decomposition of $X'X$ by calculating the eigenvalues, $D^2$, and the eigenvectors, which will be the columns of $V$:  

```{r, echo = F, warning = F, message = F}
d_5p1 <- svd_5p1$d
v_5p1 <- svd_5p1$v
u_5p1 <- svd_5p1$u

xpx <- v_5p1 %*% diag(d_5p1^2) %*% t(v_5p1)
eigen_5p1 <- eigen(xpx)

xxp <- u_5p1 %*% diag(d_5p1^2) %*% t(u_5p1)
eigen2_5p1 <- eigen(xxp)
```


\begin{displaymath}
D^2 = `r m2l(matrix(d_5p1^2, nrow = 4))`, \quad V = `r m2l(v_5p1)`
\end{displaymath}


We can find the eigen (spectral) decomposition of $XX'$ by calculating the eigenvalues, $D^2$, and the eigenvectors, which will be the columns of $U$ (rather than $V$ as for $X'X$):  

\begin{displaymath}
D^2 = `r m2l(matrix(d_5p1^2, nrow = 4))`, \quad V = `r m2l(u_5p1)`
\end{displaymath}

\textbf{Part c}  


```{r, echo = F, warning = F, message = F}
u_1 <- u_5p1[,1]
v_1 <- v_5p1[,1]
d_1 <- d_5p1[1]

x_star1 <- d_1*u_1 %*% t(v_1)
```

The best $rank=1$ approximation to $X$ will be $X^{*1} = U_1 diag(d_1)V'_1$, where $U_1$ is a matrix with the first column of $U$, and $V_1$ is a matrix with the first column of $V$:  

\begin{displaymath}
X^{*1} = U_1 diag(d_1) V'_1 = `r m2l(matrix(u_1, nrow = 5))` \times `r m2l(matrix(d_1, nrow =1))` \times `r m2l(matrix(v_1, nrow = 1)) `
\end{displaymath}

\begin{displaymath}
= `r m2l(x_star1)`
\end{displaymath}


```{r, echo = F, warning = F, message = F}
u_2 <- u_5p1[,1:2]
v_2 <- v_5p1[,1:2]
d_2 <- diag(d_5p1[1:2])

x_star2 <- u_2 %*%d_2 %*% t(v_2)
```

The best $rank=2$ approximation to $X$ will be $X^{*2} = U_2 diag(d_1, d_2)V'_2$, where $U_2$ is a matrix with the first two columns of $U$, and $V_2$ is a matrix with the first two columns of $V$:  


\begin{displaymath}
X^{*2} = U_2 diag(d_1, d_2) V'_2 = `r m2l(u_2)` \times `r m2l(d_2)` \times `r m2l(t(v_2)) `
\end{displaymath}

\begin{displaymath}
= `r m2l(x_star2)`
\end{displaymath}



\textbf{Part d}  

```{r, echo = F, warning = F, message = F}
xtilde <- scale(prob5p1_mat, center = T, scale = F)
svd_xtilde <- svd(xtilde)
```


Note that 
\begin{displaymath}
\tilde{X} = `r m2l(xtilde)`
\end{displaymath}


The Singular Value Decomposition of $\tilde{X}$ is: 

\begin{displaymath}
\tilde{U} = `r m2l(svd_xtilde$u)`, \quad \tilde{D} = `r m2l(diag(svd_xtilde$d))`
\end{displaymath}

\begin{displaymath}
\tilde{V} = `r m2l(svd_xtilde$v)`
\end{displaymath}


Therefore, the principal component directions will be the columns of $\tilde{V}$, the principal components will be the inner products $z_j = \langle x_i, v_j\rangle$, the columns of: 

```{r, echo = F, warning = F, message = F}
ztilde_mat <- xtilde %*% svd_xtilde$v
```


\begin{displaymath}
z = `r m2l(ztilde_mat)`
\end{displaymath}

The "loadings" of the first principal component are the first column of the matrix $\tilde{V}$, 
 
\begin{displaymath}
\tilde{V}_1 = `r m2l(matrix(svd_xtilde$v[,1], nrow = 4))`
\end{displaymath}

\textbf{Part e}  

The best $rank = 1$ approximation to $\tilde{X}$ will be $\tilde{X}^{*1} = \tilde{U}_1 diag(\tilde{d}_1), \tilde{V}_1$, where $\tilde{U}_1$ is a matrix with first column of $\tilde{U}$ and $\tilde{V}_1$ is a matrix with the first column of $\tilde{V}$:  

```{r, echo = F, warning =F, message = F} 
utilde_1 <- svd_xtilde$u[,1]
vtilde_1 <- svd_xtilde$v[,1]
dtilde_1 <- svd_xtilde$d[1]

xtilde_star1 <- d_1 * utilde_1 %*% t(vtilde_1)
```

\begin{displaymath}
\tilde{X}^{*1} = \tilde{U}_1 diag(\tilde{d}_1) \tilde{V}'_1 =  `r m2l(matrix(utilde_1, nrow = 5))` \times `r m2l(matrix(dtilde_1, nrow =1))` \times `r m2l(matrix(vtilde_1, nrow = 1)) `
\end{displaymath}

\begin{displaymath}
= `r m2l(xtilde_star1)`
\end{displaymath}

```{r, echo = F, warning =F, message = F} 
utilde_2 <- svd_xtilde$u[,1:2]
vtilde_2 <- svd_xtilde$v[,1:2]
dtilde_2 <- diag(svd_xtilde$d[1:2])

xtilde_star2 <-utilde_2 %*% dtilde_2 %*% t(vtilde_2)
```


The best $rank=2$ approximation to $\tilde{X}$ will be $\tilde{X}^{*2} = \tilde{U}_2 diag(\tilde{d}_1, \tilde{d}_2)\tilde{V}'_2$, where $\tilde{U}_2$ is a matrix with the first two columns of $\tilde{U}$, and $\tilde{V}_2$ is a matrix with the first two columns of $\tilde{V}$:  


\begin{displaymath}
\tilde{X}^{*2} = \tilde{U}_2 diag(\tilde{d}_1, \tilde{d}_2) \tilde{V}'_2 =  `r m2l(utilde_2)` \times `r m2l(dtilde_2)` \times `r m2l(t(vtilde_2)) `
\end{displaymath}

\begin{displaymath}
= `r m2l(xtilde_star2)`
\end{displaymath}


\textbf{Part f}  

\textbf{Part d - standardized}  

\textbf{Part e - standardized}  

\textbf{Part f - standardized}  



## Problem 2  

### Part a  
```{r, eval = TRUE, echo = FALSE, warning=FALSE, message=FALSE}
library(wavelets)
d3.4 <- read.csv(file = "data/data_3pt4.csv")

haar_basis <- function(x,m)
{
  ## define the endpoints of the interval on which we desire the basis
  a <- min(x)
  b <- max(x)
  
  ## transform the x-values to be on (0,1)
  xtrans <- (x - a)/b
  
  ## create father function
  father <- function(x)
  {
    return(1 * ( (0 < x) & (x <= 1) ) )
  }
  
  ## create mother function
  mother <- function(x)
  {
    return(father(2*x) - father(2*x - 1))
  }
  
  ## psi is a list where each element is a matrix corresponding to the basis functions corresponding to a specific m
  ## X is the design matrix
  psi <- list()
  X <- matrix(nrow = length(x), ncol = 1)
  for(i in 1:m)
  {
    psi[[m]] <- matrix(nrow = length(x), ncol = 2^m - 1)
    for(j in 1:(2^m - 1))
    {
      psi[[m]][,j] <- sqrt(2^m) * mother(2^m * (xtrans - j/(2^m)))
    }
    if(m == 1){
      X <- psi[[m]]
    }
    if(m > 1){
      X <- cbind(X, psi[[m]])
    }
  }
  
  return(X)
  
}

```


### Part b  

## Problem 3  

Note: This is Stat 602 HW3 from 2015 (problem 21)

## Problem 4  

Note: This is Stat 602 HW3 from 2015 (problem 22)

Note: There is some crossover with 502X HW2 Q15(a) here.  

### Part a   

### Part b  

### Part c  

## Problem 5  

Note: This is Stat 602 HW3 from 2015 (problem 23)

### Part a  

### Part b  

## Problem 6  
Note: This is 502X HW 2 Q16(a)  (\textbf{Kiegan will do this problem})  

```{r, echo = F, warning = F, message = F}

#p6_data <- read_csv("data/Problem3-4Data.csv")
p6_data <- data.frame(x = c(0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1), y = c(0, 1.5, 2, .5, 0, -.5, 0, 1.5, 3.5, 4.5, 3.5))

```

### Part a  

The plots for this problem can be seen in \autoref{prob6pa}.  

```{r prob6pa, echo = F, warning = F, message = F, fig.cap ="\\label{prob6pa} Plots for effective degrees of freedom 5 (top) and 9 (bottom) for Problem 6(a). "}
spline5 <- smooth.spline(x = p6_data$x, y = p6_data$y, df = 5)
spline5 <- data.frame(x = spline5$x, spline5 = spline5$y)
loess5 <- loess(y~x, data = p6_data, enp.target = 5, degree = 1)


p6_data <- p6_data %>% mutate(loess5 = loess5$fitted) %>% arrange(x) %>% full_join(spline5)


plot_5df <- p6_data %>% ggplot() + geom_point(aes(x = x, y = y)) + 
  geom_line(aes(x = x, y = spline5, color = "spline")) + 
  geom_line(aes(x = x, y = loess5, color = "loess")) + 
  scale_color_manual(name = "Fitting method", values = c("cyan", "coral"), 
                     breaks = c("spline", "loess"), aesthetics = c("colour")) + 
  theme_bw() + 
  labs(x = "X", y = "Y", title = "Approximately 5 DF")



spline9 <- smooth.spline(x = p6_data$x, y = p6_data$y, df = 9)
spline9 <- data.frame(x = spline9$x, spline9 = spline9$y)
loess9 <- loess(y~x, data = p6_data, enp.target = 9, degree = 1)


p6_data <- p6_data %>% mutate(loess9 = loess9$fitted) %>% arrange(x) %>% full_join(spline9)


plot_9df <- p6_data %>% ggplot() + geom_point(aes(x = x, y = y)) + 
  geom_line(aes(x = x, y = spline9, color = "spline")) + 
  geom_line(aes(x = x, y = loess9, color = "loess")) + 
  scale_color_manual(name = "Fitting method", values = c("cyan", "coral"), 
                     breaks = c("spline", "loess"), aesthetics = c("colour")) + 
  theme_bw() + 
  labs(x = "X", y = "Y", title = "Approximately 9 DF")


grid.arrange(plot_5df, plot_9df, ncol = 1)

```


## Problem 7  

### kNN  

### elastic net  

### PCR  

### PLS  

### MARS (in earth)  

## Problem 8  

### Part a  

### Part b  

### Part c  

### Part d  





