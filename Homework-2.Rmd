---
title: "Stat 602 Homework 2"
author: "Kiegan Rice and Nate Garton"
date: "Due 3/4/2019"
output: 
  pdf_document: 
    fig_caption: yes
header-includes: \usepackage{float, mathtools}
---

```{r global_options, echo = F, include = FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```

```{r packages, echo = F, warning = F, message = F}
library(tidyverse)
library(gridExtra)
library(knitr)
```

## Problem 1  

### Problem 4.3  

```{r prob4p3, echo = F, warning = F, message = F}
prob4p3_data <- read_csv("data/AmesHousingData.csv")
prob4p3_data <- prob4p3_data %>% select(Price, Size, Fireplace, `Bsmt Bath`, Land)

```

### Problem 4.4  

\textbf{Part a}
```{r, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE}
library(cvTools)
library(class)
library(caret)
glass <- read.table(file = "data/glass_data_4pt4.txt", sep = ",", strip.white = TRUE)[,-1]
colnames(glass) <- c("RI","Na", "Mg", "Al", "Si","K","Ca","Ba","Fe","type")

glass12 <- subset(x = glass, subset = type %in% c("1","2"))
glass12$type <- as.factor(glass12$type)
# glass12[,1:(ncol(glass12) - 1)] <- scale(x = glass12[,1:(ncol(glass12) - 1)], center = TRUE, scale = TRUE)

k <- seq(from = 1, to = 20, by = 1)
tune_par <- data.frame("k" = k)
fitcontrol <- trainControl(method = "cv", number = 10)
knn_results <- caret::train(form = type ~ ., 
                            data = glass12, 
                            method = "knn", 
                            preProcess = c("center","scale"), 
                            tuneGrid = tune_par,
                            trControl = fitcontrol)

knn_results
```

Based on 10-fold cross-validation, the accuracy is estimated to be highest when $k = 4$.

\textbf{Part b}

The classification rule $\hat{y} = 2 I\left[t(x) \geq k/2\right] + I\left[t(x) < k/2\right]$ is equivalent to $\hat{y} = 2 I\left[t(x)/k \geq 1/2\right] + I\left[t(x)/k < 1/2\right]$. Here, $t(x)/k$ is an estimate of $E\left[ \text{type} = 2|x \right] = P(\text{type} = 2|x)$. Thus, this is directly an estimate of a posterior class probability which accounts for the prior class probabilities. This means that no modification of kNN needs to be made to account for differing prior probabilities.

### Problem 5.1  

```{r prob5p1, echo = F, warning = F, message = F}
prob5p1_mat <- matrix(c(2, 4, 7, 2, 
                        4, 3, 5, 5, 
                        3, 4, 6, 1, 
                        5, 2, 4, 2, 
                        1, 3, 4, 4), nrow = 5, byrow = T)

#Note: Most of this is on 602 HW2 from 2015 (problem 6)  
```

The following are decompositions using the matrix

$$
X = 
\begin{bmatrix}
2 & 4 & 7 & 2 \\
4 & 3 & 5 & 5\\
3 & 4 & 6 & 1 \\
5 & 2 & 4 & 2 \\
1 & 3 & 4 & 4 \\
\end{bmatrix}
$$

\textbf{Part a}  

```{r, echo = F, warning = F, message = F, results='asis'}
qr_5p1 <- qr(prob5p1_mat)
qr_bases <- qr.Q(qr_5p1)
qr_r <- qr.R(qr_5p1)


svd_5p1 <- svd(prob5p1_mat)
```

The QR decomposition of $X$ is: 

$$
Q = 
\begin{bmatrix}
-0.27 & 0.57 & 0.77 & 0.04 \\
-0.54 & -0.07 & -0.13 & 0.56 \\
-0.40 & 0.37 & -0.35 & -0.71\\
-0.67 & -0.50 & 0.10 & -0.13\\
-0.13 & 0.53 & -0.50 & 0.40\\
\end{bmatrix}, \quad \quad R = 
\begin{bmatrix}
-7.42 & -6.07 & -10.25 & -5.53 \\
0 & 4.14 & 5.99 & 2.28 \\
0 & 0 & 1.06 & -1.23 \\
0 & 0 & 0 & 3.57 \\
\end{bmatrix}
$$
Thus, the basis for $C(X)$ will be the columns of the matrix $Q$.  

The Singular Value Decomposition of $X$ is:  

$$
U = 
\begin{bmatrix}
-0.50 & 0.53 & 0.16 & -0.66  \\
-0.50 & -0.59 & 0.13 & -0.11 \\
-0.45 & 0.49 & -0.25 & 0.65 \\
-0.39 & -0.32 & -0.68 & -0.08 \\
-0.36 & -0.17 & 0.65 & 0.35 \\
\end{bmatrix}, 
\quad D = 
\begin{bmatrix}
16.58 & 0 & 0 & 0 \\
0 & 3.78 & 0 & 0 \\
0 & 0 & 3.38 & 0 \\
0 & 0 & 0 & 0.55 \\
\end{bmatrix},
$$

$$
V = 
\begin{bmatrix}
-0.40 & -0.43 & -0.80 & 0.12  \\
-0.44 & 0.30 & 0.18 & 0.83 \\
-0.71 & 0.45 & 0.04 & -0.54 \\
-0.37 & -0.72 & 0.57 & -0.06 \\
\end{bmatrix}
$$
Thus, the basis for $C(X)$ will be the columns of the matrix $U$.  


\textbf{Part b}  

We can find the eigen (spectral) decomposition of $X'X$ by calculating the eigenvalues, $D^2$, and the eigenvectors, which will be the columns of $V$:  
```{r, echo = F, warning = F, message = F}
d_5p1 <- svd_5p1$d
v_5p1 <- svd_5p1$v
u_5p1 <- svd_5p1$u

xpx <- v_5p1 %*% diag(d_5p1^2) %*% t(v_5p1)
eigen_5p1 <- eigen(xpx)

xxp <- u_5p1 %*% diag(d_5p1^2) %*% t(u_5p1)
eigen2_5p1 <- eigen(xxp)
```

$$
D^2 = 
\begin{bmatrix}
274.94 \\
14.32 \\
11.44\\
 0.30 \\
\end{bmatrix}, \quad
V = 
\begin{bmatrix}
-0.40 & -0.43 & -0.80 & 0.12  \\
-0.44 & 0.30 & 0.18 & 0.83 \\
-0.71 & 0.45 & 0.04 & -0.54 \\
-0.37 & -0.72 & 0.57 & -0.06 \\
\end{bmatrix}
$$


We can find the eigen (spectral) decomposition of $XX'$ by calculating the eigenvalues, $D^2$, and the eigenvectors, which will be the columns of $U$ (rather than $V$ as for $X'X$):  

$$
D^2 = 
\begin{bmatrix}
274.94 \\
14.32 \\
11.44\\
 0.30 \\
\end{bmatrix}, \quad
U = 
\begin{bmatrix}
-0.50 & 0.53 & 0.16 & -0.66  \\
-0.50 & -0.59 & 0.13 & -0.11 \\
-0.45 & 0.49 & -0.25 & 0.65 \\
-0.39 & -0.32 & -0.68 & -0.08 \\
-0.36 & -0.17 & 0.65 & 0.35 \\
\end{bmatrix}
$$


\textbf{Part c}  


```{r, echo = F, warning = F, message = F}
u_1 <- u_5p1[,1]
v_1 <- v_5p1[,1]
d_1 <- d_5p1[1]

x_star1 <- d_1*u_1 %*% t(v_1)
```

The best $rank=1$ approximation to $X$ will be $X^{*1} = U_1 diag(d_1)V'_1$, where $U_1$ is a matrix with the first column of $U$, and $V_1$ is a matrix with the first column of $V$:  

$$
X^{*1} = U_1 diag(d_1) V'_1 = 
\begin{bmatrix}
-0.50 \\
-0.50 \\
-0.45 \\
-0.39\\
-0.36 \\
\end{bmatrix} 
\times [16.58] \times 
\begin{bmatrix}
-0.40 & -0.44  & -0.71 & -0.37 \\
\end{bmatrix}
$$

$$
=
\begin{bmatrix}
3.35 & 3.61 & 5.88 & 3.11 \\
3.38 & 3.64 & 5.94 & 3.13 \\
3.08 & 3.31 & 5.40 & 2.85 \\
2.62 & 2.83 & 4.61 & 2.43 \\
2.45 & 2.64 & 4.31 & 2.27 \\
\end{bmatrix}
$$



```{r, echo = F, warning = F, message = F}
u_2 <- u_5p1[,1:2]
v_2 <- v_5p1[,1:2]
d_2 <- diag(d_5p1[1:2])

x_star2 <- u_2 %*%d_2 %*% t(v_2)
```

The best $rank=2$ approximation to $X$ will be $X^{*2} = U_2 diag(d_1, d_2)V'_2$, where $U_2$ is a matrix with the first two columns of $U$, and $V_2$ is a matrix with the first two columns of $V$:  

$$
X^{*2} = U_2 diag(d_1, d_2) V'_2 = 
\begin{bmatrix}
-0.50 & 0.53 \\
-0.50 & -0.59\\
-0.45 & 0.49 \\
-0.39 & -0.32\\
-0.36 & -0.17\\
\end{bmatrix} 
\times 
\begin{bmatrix}
16.58 & 0 \\
0 & 3.78 \\
\end{bmatrix}
\times 
\begin{bmatrix}
-0.40 & -0.44  & -0.71 & -0.37 \\
-0.43 & 0.30 & 0.45 & -0.72 \\
\end{bmatrix}
$$
$$
=
\begin{bmatrix}
2.48 & 4.20 & 6.78 & 1.66 \\
4.34 & 2.97 & 4.95 & 4.75 \\
2.28 & 3.86 & 6.23 & 1.51 \\
3.16 & 2.46 & 4.07 & 3.33 \\
2.73 & 2.44 & 4.01 & 2.74 \\
\end{bmatrix}
$$


\textbf{Part d}  

Note that 
$$
\tilde{X} = 
\begin{bmatrix}
-1 & 0.8 & 1.8 & -0.8  \\
1 & -0.2 & -0.2 & 2.2  \\
0 & 0.8 & 0.8 & -1.8   \\
2 & -1.2 & -1.2 & -0.8 \\
-2 & -0.2 & -1.2 & 1.2 \\
\end{bmatrix}
$$


```{r, echo = F, warning = F, message = F}
xtilde <- scale(prob5p1_mat, center = T, scale = F)
svd_xtilde <- svd(xtilde)
```



The Singular Value Decomposition of $\tilde{X}$ is: 

$$
\tilde{U} = 
\begin{bmatrix}
-0.57 & 0.16 & 0.28 & -0.60  \\
-0.51 & 0.11 & 0.69 & 0.21 \\
-0.50 & -0.25 & -0.09 & 0.69 \\
0.34 & -0.68 & -0.32 & -0.33 \\
0.22 & 0.65 & -0.56 & 0.03 \\
\end{bmatrix}, 
\quad \tilde{D} = 
\begin{bmatrix}
3.83 & 0 & 0 & 0 \\
0 & 3.38 & 0 & 0 \\
0 & 0 & 2.01 & 0 \\
0 & 0 & 0 & 0.48 \\
\end{bmatrix},
$$
$$
\tilde{V} = 
\begin{bmatrix}
0.34 & -0.81 & 0.45 & 0.18  \\
-0.36 & 0.18 & 0.26 & 0.87 \\
-0.57 & 0.03 & 0.68 & -0.45 \\
0.64 & 0.56 & 0.52 & 0.00 \\
\end{bmatrix}
$$
Therefore, the principal component directions will be the columns of $\tilde{V}$, the principal components will be the inner products $z_j = \langle x_i, v_j\rangle$, the columns of: 

$$
z = 
\begin{bmatrix}
-2.19 & 0.56 & -.57 & -0.29 \\
1.95 & 0.39 & 1.40 & 0.10 \\
-1.91 & -0.84 & -0.18 & 0.33 \\
1.30 & -2.32 & -0.65 & -0.16 \\
0.85 & 2.21 & -1.14 & 0.01 \\
\end{bmatrix}
$$
 
The "loadings" of the first principal component are the first column of the matrix $\tilde{V}$, 
 
$$
\tilde{V_1} = 
\begin{bmatrix}
0.34 \\
-0.37 \\
-0.57 \\
0.64
\end{bmatrix}
$$

\textbf{Part e}  

The best $rank = 1$ approximation to $\tilde{X}$ will be $\tilde{X}^{*1} = \tilde{U}_1 diag(\tilde{d}_1), \tilde{V}_1$, where $\tilde{U}_1$ is a matrix with first column of $\tilde{U}$ and $\tilde{V}_1$ is a matrix with the first column of $\tilde{V}$:  

```{r, echo = F, warning =F, message = F} 
utilde_1 <- svd_xtilde$u[,1]
vtilde_1 <- svd_xtilde$v[,1]
dtilde_1 <- svd_xtilde$d[1]

xtilde_star1 <- d_1 * utilde_1 %*% t(vtilde_1)
```

$$
\tilde{X}^{*1} = \tilde{U}_1 diag(\tilde{d}_1) \tilde{V}'_1 = 
\begin{bmatrix}
-0.57 \\
-0.51 \\
-0.50 \\
-0.34\\
-0.22 \\
\end{bmatrix} 
\times [3.83] \times 
\begin{bmatrix}
0.34 & -0.37  & -0.57 & 0.64 \\
\end{bmatrix}
$$


$$
=
\begin{bmatrix}
-3.25 & 3.49 & 5.45 & -6.10 \\
2.90 & -3.10 & -4.85 & 5.44 \\
-2.85 & 3.05 & 4.76 & -5.34 \\
1.94 & -2.08 & -3.24 & 3.64 \\
1.26 & -1.35 & -2.11 & 2.37 \\
\end{bmatrix}
$$



```{r, echo = F, warning =F, message = F} 
utilde_2 <- svd_xtilde$u[,1:2]
vtilde_2 <- svd_xtilde$v[,1:2]
dtilde_2 <- diag(svd_xtilde$d[1:2])

xtilde_star2 <-utilde_2 %*% dtilde_2 %*% t(vtilde_2)
```


The best $rank=2$ approximation to $\tilde{X}$ will be $\tilde{X}^{*2} = \tilde{U}_2 diag(\tilde{d}_1, \tilde{d}_2)\tilde{V}'_2$, where $\tilde{U}_2$ is a matrix with the first two columns of $\tilde{U}$, and $\tilde{V}_2$ is a matrix with the first two columns of $\tilde{V}$:  

$$
\tilde{X}^{*2} = \tilde{U}_2 diag(\tilde{d}_1, \tilde{d}_2) \tilde{V}'_2 = 
\begin{bmatrix}
-0.57 & 0.16 \\
0.51 & 0.11\\
-0.50 & -0.25 \\
0.34 & -0.68\\
0.22 & 0.65\\
\end{bmatrix} 
\times 
\begin{bmatrix}
3.83 & 0 \\
0 & 3.38 \\
\end{bmatrix}
\times 
\begin{bmatrix}
0.34 & -0.37  & -0.57 & 0.64 \\
-0.81 & 0.18 & 0.03 & 0.56 \\
\end{bmatrix}
$$
$$
=
\begin{bmatrix}
-1.20 & 0.90 & 1.28 & -1.09 \\
0.36 & -0.65 & -1.10 & 1.47 \\
0.02 & 0.55 & 1.07 & -1.71 \\
2.31 & -0.89 & -0.83 & -0.46 \\
-1.49 & 0.08 & -0.41 & 1.79 \\
\end{bmatrix}
$$


\textbf{Part f}  

\textbf{Part d - standardized}  

\textbf{Part e - standardized}  

\textbf{Part f - standardized}  



## Problem 2  

### Part a  
```{r, eval = TRUE, echo = FALSE, warning=FALSE, message=FALSE}
library(wavelets)
d3.4 <- read.csv(file = "data/data_3pt4.csv")

haar_basis <- function(x,m)
{
  ## define the endpoints of the interval on which we desire the basis
  a <- min(x)
  b <- max(x)
  
  ## transform the x-values to be on (0,1)
  xtrans <- (x - a)/b
  
  ## create father function
  father <- function(x)
  {
    return(1 * ( (0 < x) & (x <= 1) ) )
  }
  
  ## create mother function
  mother <- function(x)
  {
    return(father(2*x) - father(2*x - 1))
  }
  
  ## psi is a list where each element is a matrix corresponding to the basis functions corresponding to a specific m
  ## X is the design matrix
  psi <- list()
  X <- matrix(nrow = length(x), ncol = 1)
  for(i in 1:m)
  {
    psi[[m]] <- matrix(nrow = length(x), ncol = 2^m - 1)
    for(j in 1:(2^m - 1))
    {
      psi[[m]][,j] <- sqrt(2^m) * mother(2^m * (xtrans - j/(2^m)))
    }
    if(m == 1){
      X <- psi[[m]]
    }
    if(m > 1){
      X <- cbind(X, psi[[m]])
    }
  }
  
  return(X)
  
}

```


### Part b  

## Problem 3  

Note: This is Stat 602 HW3 from 2015 (problem 21)

## Problem 4  

Note: This is Stat 602 HW3 from 2015 (problem 22)

Note: There is some crossover with 502X HW2 Q15(a) here.  

### Part a   

### Part b  

### Part c  

## Problem 5  

Note: This is Stat 602 HW3 from 2015 (problem 23)

### Part a  

### Part b  

## Problem 6  
Note: This is 502X HW 2 Q16(a)  (\textbf{Kiegan will do this problem})  

```{r, echo = F, warning = F, message = F}

#p6_data <- read_csv("data/Problem3-4Data.csv")
p6_data <- data.frame(x = c(0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1), y = c(0, 1.5, 2, .5, 0, -.5, 0, 1.5, 3.5, 4.5, 3.5))

```

### Part a  

The plots for this problem can be seen in \autoref{prob6pa}.  

```{r prob6pa, echo = F, warning = F, message = F, fig.cap ="\\label{prob6pa} Plots for effective degrees of freedom 5 (top) and 9 (bottom) for Problem 6(a). "}
spline5 <- smooth.spline(x = p6_data$x, y = p6_data$y, df = 5)
spline5 <- data.frame(x = spline5$x, spline5 = spline5$y)
loess5 <- loess(y~x, data = p6_data, enp.target = 5, degree = 1)


p6_data <- p6_data %>% mutate(loess5 = loess5$fitted) %>% arrange(x) %>% full_join(spline5)


plot_5df <- p6_data %>% ggplot() + geom_point(aes(x = x, y = y)) + 
  geom_line(aes(x = x, y = spline5, color = "spline")) + 
  geom_line(aes(x = x, y = loess5, color = "loess")) + 
  scale_color_manual(name = "Fitting method", values = c("cyan", "coral"), 
                     breaks = c("spline", "loess"), aesthetics = c("colour")) + 
  theme_bw() + 
  labs(x = "X", y = "Y", title = "Approximately 5 DF")



spline9 <- smooth.spline(x = p6_data$x, y = p6_data$y, df = 9)
spline9 <- data.frame(x = spline9$x, spline9 = spline9$y)
loess9 <- loess(y~x, data = p6_data, enp.target = 9, degree = 1)


p6_data <- p6_data %>% mutate(loess9 = loess9$fitted) %>% arrange(x) %>% full_join(spline9)


plot_9df <- p6_data %>% ggplot() + geom_point(aes(x = x, y = y)) + 
  geom_line(aes(x = x, y = spline9, color = "spline")) + 
  geom_line(aes(x = x, y = loess9, color = "loess")) + 
  scale_color_manual(name = "Fitting method", values = c("cyan", "coral"), 
                     breaks = c("spline", "loess"), aesthetics = c("colour")) + 
  theme_bw() + 
  labs(x = "X", y = "Y", title = "Approximately 9 DF")


grid.arrange(plot_5df, plot_9df, ncol = 1)

```


## Problem 7  

### kNN  

### elastic net  

### PCR  

### PLS  

### MARS (in earth)  

## Problem 8  

### Part a  

### Part b  

### Part c  

### Part d  





