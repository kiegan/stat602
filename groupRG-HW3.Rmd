---
title: "Stat 602 Homework 3"
author: "Kiegan Rice and Nate Garton"
date: "Due ??"
output: 
  pdf_document: 
    fig_caption: yes
header-includes: \usepackage{float, mathtools, accents}
---


\newcommand{\dbtilde}[1]{\accentset{\approx}{#1}}

```{r global_options, echo = F, include = FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```

```{r packages, echo = F, warning = F, message = F}
library(tidyverse)
library(gridExtra)
library(knitr)
library(cvTools)
library(class)
library(caret)
library(elasticnet)
library(earth)
library(pls)
library(reshape2)
library(kernlab)
library(randomForest)
library(nnet)
library(rpart)
library(C50)
library(glmnet)
library(klaR)
library(Cubist)
library(xgboost)
library(caretEnsemble)
```

# Problem 9  
kiegan will do this problem 

```{r, echo = F, warning = F, message = F}
wines <- read_delim("data/winequality-white.csv", delim = ";")

## below is code from HW2... need to change "cv" to "repeatedcv" method.


# k nearest neighbors
wineControl <- trainControl(method = "cv", number = 10)

wine_knn <- caret::train(form = quality~., data = wines, 
                       method = "knn", preProcess = c("center", "scale"), 
                       trControl = wineControl, 
                       tuneLength = 20)

preds_knn <- predict(wine_knn, newdata = wines)


# elastic net
wine_enet <- caret::train(form = quality~., data = wines, 
                       method = "enet", preProcess = c("center", "scale"), 
                       trControl = wineControl, 
                       tuneLength = 20)

preds_enet <- predict(wine_enet, newdata = wines)

# principal components regression
wine_pcr <- caret::train(form = quality~., data = wines, 
                       method = "pcr", preProcess = c("center", "scale"), 
                       trControl = wineControl, 
                       tuneLength = 20)

preds_pcr <- predict(wine_pcr, newdata = wines)


# partial least squares 
wine_pls <- caret::train(form = quality~., data = wines, 
                       method = "pls", preProcess = c("center", "scale"), 
                       trControl = wineControl, 
                       tuneLength = 20)

preds_pls <- predict(wine_pls, newdata = wines)


# MARS
#wine_mars <- caret::train(form = quality~., data = wines, 
#                       method = "earth", preProcess = c("center", "scale"), 
#                       trControl = wineControl, 
#                       tuneLength = 20)

#preds_mars <- predict(wine_mars, newdata = wines)
#saveRDS(preds_mars, "data/preds_mars")

## This takes a long time to re-render, so I saved and re-read in the predictions to save computing whenever we render the document.
preds_mars <- readRDS("data/preds_mars")

wine_preds <- data.frame(quality = wines$quality, knn = preds_knn, 
                         enet = preds_enet, pcr = preds_pcr,
                         pls = preds_pls, mars = preds_mars[,1])



preds_mars <- readRDS("data/preds_mars")

wines_preds <- data.frame(quality = wines$quality, knn = preds_knn, 
                         enet = preds_enet, pcr = preds_pcr,
                         pls = preds_pls, mars = preds_mars[,1])
```

# Problem 10  
Shown below is the matrix of the proportion of predictions that agree between any two methods. The row and column labeled "actual" corresponds to the true data label.  

```{r, eval = TRUE, echo = TRUE, results = 'asis', message=FALSE, warning=FALSE}
wine <- read.csv(file = "data/winequality-white.csv", sep = ";", check.names = TRUE)
y <- -1*(wine$quality <= 7) + 1*(wine$quality > 7)
wine$y <- as.factor(y)
train_wine <- wine[,-(ncol(wine) - 1)]

fitControl <- trainControl(method = "repeatedcv", 
                           number = 10, repeats = 5, search = "grid")
# 
# set.seed(1308)
mods <- c("knn", "nnet", "rpart","rf", 
          "xgbTree", "C5.0","glmnet", "stepLDA", 
          "svmLinear", "svmPoly", "svmRadial")
# preds <- list()
# fitmod <- list()
# for(i in 1:length(mods))
# {
#   fitmod[[i]] <- caret::train(y ~ ., data = train_wine, trControl = fitControl, method = mods[i])
# }
# names(fitmod) <- mods
# 
# saveRDS(fitmod, file = "hw3_10_models.rda")
fitmod <- readRDS(file = "hw3_10_models.rda")
preds <- extractPrediction(models = fitmod)
pred_y <- data.frame("obs" = train_wine$y, "pred" = train_wine$y, 
                     "model" = "actual", "dataType" = "Training", "object" = "none")

preds <- rbind(preds, pred_y)

pred_mat <- matrix(nrow = length(mods) + 1, ncol = length(mods) + 1)
mods[12] <- "actual"
for(i in 1:nrow(pred_mat))
{
  for(j in 1:nrow(pred_mat))
  {
    pred_mat[i,j] <- mean(preds[preds$model == mods[i],]$pred == preds[preds$model == mods[j],]$pred)
  }
}
colnames(pred_mat) <- mods
rownames(pred_mat) <- mods
knitr::kable(round(pred_mat[,-ncol(pred_mat)], digits = 3))
```

# Problem 11  
kiegan will do this problem

# Problem 12  
The predictors from the boosted tree, kNN, and C5.0 are all better than the worst predictors (which all had tied training error of 0.963), and seem to be the least correlated with one another. Therefore, we will include these three predictors in the generalized stacking model. The best cv error from the stacked model was $\approx 0.976$ and the best individual model had a best cv error of $\approx 0.974$. The two scores are very close, which is to be expected, as the correlation of the predictions between each of these models was still very high.

```{r, eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
set.seed(1308)
train_wine$target <- train_wine$y
levels(train_wine$target) <- c("bad", "good") 
train_wine <- train_wine[,-(ncol(train_wine) - 1)]
# fitControl <- trainControl(method = "repeatedcv", 
#                            number = 10, repeats = 5, search = "grid", classProbs = TRUE)
# clist <- caretList(target ~ .,data = train_wine,
#                    trControl = fitControl,
#                    methodList = c("xgbTree", "C5.0", "knn"))
# saveRDS(clist, file = "hw3_12_mod_list.rda")

clist <- readRDS(file = "hw3_12_mod_list.rda")

cverror <- c(clist$xgbTree$results[108,]$Accuracy,
             clist$C5.0$results[9,]$Accuracy,
             clist$knn$results[1,]$Accuracy)
stackmod <- caretStack(all.models = clist, 
                       method = "rf",
                       metric = "Accuracy",
                       trControl = trainControl(
                         method = "cv",
                         number = 10, 
                         search = "grid"
                       ))

cverror <- c(cverror, stackmod$error[1,]$Accuracy)
names(cverror) <- c("xgbTree","C5.0","knn", "stack")
print(cverror)
```


